{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88faef1a-88c0-4ba8-b27d-0a0ac5e84bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 14:23:07.220843: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-27 14:23:07.257348: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-27 14:23:07.257379: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-27 14:23:07.258865: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-27 14:23:07.265928: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-27 14:23:07.858911: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import json\n",
    "from keras import layers, models, optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    InputLayer,\n",
    "    LSTM,\n",
    "    Bidirectional,\n",
    "    Embedding,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    Convolution1D,\n",
    "    MaxPooling1D,\n",
    "    BatchNormalization,\n",
    ")\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e063d2b8-24a6-46f7-be1a-d6cf333a1df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_src import filter_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5df9da30-f725-4c55-ae8d-93406ec01956",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/ready_data/train_p2.csv\")\n",
    "TEST_DATASET = df[df[\"source\"] == \"pdb2272\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1476ff4b-50bb-469b-aeb7-ec6d383cc7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_path: str, name: str, mode: str = \"train\") -> Tuple[List[str], List[int]]:\n",
    "    dataset = pd.read_csv(data_path)\n",
    "    dataset = dataset[dataset[\"source\"] == name]\n",
    "    if mode == \"train\":\n",
    "        dataset = filter_train(dataset, TEST_DATASET)\n",
    "    dataset = dataset.sample(frac=1).reset_index(drop=True)  # shuffle the dataset\n",
    "    return list(dataset[\"sequence\"]), list(dataset[\"label\"])\n",
    "\n",
    "\n",
    "def split_dataset(\n",
    "    sequences_list: List[str], labels_list: List[int], train_size: float = 0.8\n",
    ") -> Tuple[List[str], List[str], List[str], List[int], List[int], List[int]]:\n",
    "    dataset = pd.DataFrame({\"sequence\": sequences_list, \"label\": labels_list})\n",
    "    dataset = dataset.sample(frac=1, random_state=1)\n",
    "    train, remaining = train_test_split(dataset, train_size=train_size, random_state=2)\n",
    "    valid, test = train_test_split(remaining, test_size=0.5, random_state=3)\n",
    "    x_train, x_valid, x_test = train[\"sequence\"], valid[\"sequence\"], test[\"sequence\"]\n",
    "    y_train, y_valid, y_test = train[\"label\"], valid[\"label\"], test[\"label\"]\n",
    "    return (\n",
    "        list(x_train),\n",
    "        list(x_valid),\n",
    "        list(x_test),\n",
    "        list(y_train),\n",
    "        list(y_valid),\n",
    "        list(y_test),\n",
    "    )\n",
    "\n",
    "def one_hot_encoding(\n",
    "    sequence: str,\n",
    "    max_seq_length: int = 800,\n",
    "    CONSIDERED_AA: str = \"ACDEFGHIKLMNPQRSTVWY\",\n",
    "):\n",
    "    # adapt sequence size\n",
    "    if len(sequence) > max_seq_length:\n",
    "        # short the sequence\n",
    "        sequence = sequence[:max_seq_length]\n",
    "    else:\n",
    "        # pad the sequence\n",
    "        sequence = sequence + \".\" * (max_seq_length - len(sequence))\n",
    "\n",
    "    # encode sequence\n",
    "    encoded_sequence = np.zeros((max_seq_length, len(CONSIDERED_AA)))  # (1000, 20)\n",
    "    for i, amino_acid in enumerate(sequence):\n",
    "        if amino_acid in CONSIDERED_AA:\n",
    "            encoded_sequence[i][CONSIDERED_AA.index(amino_acid)] = 1\n",
    "    model_input = np.expand_dims(encoded_sequence, 0)  # add batch dimension\n",
    "\n",
    "    return model_input  # (1, 1000, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2f945a3-a348-45e2-a077-e553e7b39c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall_keras = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall_keras\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision_keras = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision_keras\n",
    "\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n",
    "    return tn / (tn + fp + K.epsilon())\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
    "\n",
    "def matthews_correlation_coefficient(y_true, y_pred):\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true * (1 - y_pred), 0, 1)))\n",
    "\n",
    "    num = tp * tn - fp * fn\n",
    "    den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "    return num / K.sqrt(den + K.epsilon())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d8d970d-0d2c-4073-9cab-786f9dcdbc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend import *\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "\n",
    "#own_batch_dot = batch_dot  # force standard implementation \n",
    "\n",
    "# import of batch_dot operation from TF 1.13\n",
    "# https://github.com/tensorflow/tensorflow/blob/v1.13.1/tensorflow/python/keras/backend.py\n",
    "\n",
    "def own_batch_dot(x, y, axes=None):\n",
    "  \"\"\"Batchwise dot product.\n",
    "  `batch_dot` is used to compute dot product of `x` and `y` when\n",
    "  `x` and `y` are data in batch, i.e. in a shape of\n",
    "  `(batch_size, :)`.\n",
    "  `batch_dot` results in a tensor or variable with less dimensions\n",
    "  than the input. If the number of dimensions is reduced to 1,\n",
    "  we use `expand_dims` to make sure that ndim is at least 2.\n",
    "  Arguments:\n",
    "      x: Keras tensor or variable with `ndim >= 2`.\n",
    "      y: Keras tensor or variable with `ndim >= 2`.\n",
    "      axes: list of (or single) int with target dimensions.\n",
    "          The lengths of `axes[0]` and `axes[1]` should be the same.\n",
    "  Returns:\n",
    "      A tensor with shape equal to the concatenation of `x`'s shape\n",
    "      (less the dimension that was summed over) and `y`'s shape\n",
    "      (less the batch dimension and the dimension that was summed over).\n",
    "      If the final rank is 1, we reshape it to `(batch_size, 1)`.\n",
    "  Examples:\n",
    "      Assume `x = [[1, 2], [3, 4]]` and `y = [[5, 6], [7, 8]]`\n",
    "      `batch_dot(x, y, axes=1) = [[17, 53]]` which is the main diagonal\n",
    "      of `x.dot(y.T)`, although we never have to calculate the off-diagonal\n",
    "      elements.\n",
    "      Shape inference:\n",
    "      Let `x`'s shape be `(100, 20)` and `y`'s shape be `(100, 30, 20)`.\n",
    "      If `axes` is (1, 2), to find the output shape of resultant tensor,\n",
    "          loop through each dimension in `x`'s shape and `y`'s shape:\n",
    "      * `x.shape[0]` : 100 : append to output shape\n",
    "      * `x.shape[1]` : 20 : do not append to output shape,\n",
    "          dimension 1 of `x` has been summed over. (`dot_axes[0]` = 1)\n",
    "      * `y.shape[0]` : 100 : do not append to output shape,\n",
    "          always ignore first dimension of `y`\n",
    "      * `y.shape[1]` : 30 : append to output shape\n",
    "      * `y.shape[2]` : 20 : do not append to output shape,\n",
    "          dimension 2 of `y` has been summed over. (`dot_axes[1]` = 2)\n",
    "      `output_shape` = `(100, 30)`\n",
    "  ```python\n",
    "      >>> x_batch = K.ones(shape=(32, 20, 1))\n",
    "      >>> y_batch = K.ones(shape=(32, 30, 20))\n",
    "      >>> xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=[1, 2])\n",
    "      >>> K.int_shape(xy_batch_dot)\n",
    "      (32, 1, 30)\n",
    "  ```\n",
    "  \"\"\"\n",
    "  if isinstance(axes, int):\n",
    "    axes = (axes, axes)\n",
    "  x_ndim = ndim(x)\n",
    "  y_ndim = ndim(y)\n",
    "  if axes is None:\n",
    "    # behaves like tf.batch_matmul as default\n",
    "    axes = [x_ndim - 1, y_ndim - 2]\n",
    "  if x_ndim > y_ndim:\n",
    "    diff = x_ndim - y_ndim\n",
    "    y = array_ops.reshape(y,\n",
    "                          array_ops.concat(\n",
    "                              [array_ops.shape(y), [1] * (diff)], axis=0))\n",
    "  elif y_ndim > x_ndim:\n",
    "    diff = y_ndim - x_ndim\n",
    "    x = array_ops.reshape(x,\n",
    "                          array_ops.concat(\n",
    "                              [array_ops.shape(x), [1] * (diff)], axis=0))\n",
    "  else:\n",
    "    diff = 0\n",
    "  if ndim(x) == 2 and ndim(y) == 2:\n",
    "    if axes[0] == axes[1]:\n",
    "      out = math_ops.reduce_sum(math_ops.multiply(x, y), axes[0])\n",
    "    else:\n",
    "      out = math_ops.reduce_sum(\n",
    "          math_ops.multiply(array_ops.transpose(x, [1, 0]), y), axes[1])\n",
    "  else:\n",
    "    adj_x = None if axes[0] == ndim(x) - 1 else True\n",
    "    adj_y = True if axes[1] == ndim(y) - 1 else None\n",
    "    out = math_ops.matmul(x, y, adjoint_a=adj_x, adjoint_b=adj_y)\n",
    "  if diff:\n",
    "    if x_ndim > y_ndim:\n",
    "      idx = x_ndim + y_ndim - 3\n",
    "    else:\n",
    "      idx = x_ndim - 1\n",
    "    out = array_ops.squeeze(out, list(range(idx, idx + diff)))\n",
    "  if ndim(out) == 1:\n",
    "    out = expand_dims(out, 1)\n",
    "  return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90de629f-c83d-4805-87f2-f5858925d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import initializers, layers\n",
    "\n",
    "class Length(layers.Layer):\n",
    "    \"\"\"\n",
    "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss.\n",
    "    Using this layer as model's output can directly predict labels by using `y_pred = np.argmax(model.predict(x), 1)`\n",
    "    inputs: shape=[None, num_vectors, dim_vector]\n",
    "    output: shape=[None, num_vectors]\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.sqrt(K.sum(K.square(inputs), -1))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]\n",
    "\n",
    "\n",
    "class Mask(layers.Layer):\n",
    "    \"\"\"\n",
    "    Mask a Tensor with shape=[None, num_capsule, dim_vector] either by the capsule with max length or by an additional \n",
    "    input mask. Except the max-length capsule (or specified capsule), all vectors are masked to zeros. Then flatten the\n",
    "    masked Tensor.\n",
    "    For example:\n",
    "        ```\n",
    "        x = keras.layers.Input(shape=[8, 3, 2])  # batch_size=8, each sample contains 3 capsules with dim_vector=2\n",
    "        y = keras.layers.Input(shape=[8, 3])  # True labels. 8 samples, 3 classes, one-hot coding.\n",
    "        out = Mask()(x)  # out.shape=[8, 6]\n",
    "        # or\n",
    "        out2 = Mask()([x, y])  # out2.shape=[8,6]. Masked with true labels y. Of course y can also be manipulated.\n",
    "        ```\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if type(inputs) is list:  # true label is provided with shape = [None, n_classes], i.e. one-hot code.\n",
    "            assert len(inputs) == 2\n",
    "            inputs, mask = inputs\n",
    "        else:  # if no true label, mask by the max length of capsules. Mainly used for prediction\n",
    "            # compute lengths of capsules\n",
    "            x = K.sqrt(K.sum(K.square(inputs), -1))\n",
    "            # generate the mask which is a one-hot code.\n",
    "            # mask.shape=[None, n_classes]=[None, num_capsule]\n",
    "            mask = K.one_hot(indices=K.argmax(x, 1), num_classes=tf.shape(x)[1])\n",
    "\n",
    "        # inputs.shape=[None, num_capsule, dim_capsule]\n",
    "        # mask.shape=[None, num_capsule]\n",
    "        # masked.shape=[None, num_capsule * dim_capsule]\n",
    "        masked = K.batch_flatten(inputs * K.expand_dims(mask, -1))\n",
    "        return masked\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if type(input_shape[0]) is tuple:  # true label provided\n",
    "            return tuple([None, input_shape[0][1] * input_shape[0][2]])\n",
    "        else:  # no true label provided\n",
    "            return tuple([None, input_shape[1] * input_shape[2]])\n",
    "\n",
    "\n",
    "def squash(vectors, axis=-1):\n",
    "    \"\"\"\n",
    "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "    :param vectors: some vectors to be squashed, N-dim tensor\n",
    "    :param axis: the axis to squash\n",
    "    :return: a Tensor with same shape as input vectors\n",
    "    \"\"\"\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors\n",
    "\n",
    "\n",
    "class CapsuleLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n",
    "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
    "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape = \\\n",
    "    [None, num_capsule, dim_capsule]. For Dense Layer, input_dim_capsule = dim_capsule = 1.\n",
    "    \n",
    "    :param num_capsule: number of capsules in this layer\n",
    "    :param dim_capsule: dimension of the output vectors of the capsules in this layer\n",
    "    :param num_routing: number of iterations for the routing algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, num_capsule, dim_capsule, num_routing=3,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.num_routing = num_routing\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\"\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_capsule = input_shape[2]\n",
    "\n",
    "        # Transform matrix\n",
    "        self.W = self.add_weight(shape=(self.num_capsule, self.input_num_capsule,\n",
    "                                        self.dim_capsule, self.input_dim_capsule),\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='W')\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs.shape=[None, input_num_capsule, input_dim_capsule]\n",
    "        # inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule]\n",
    "        inputs_expand = K.expand_dims(inputs, 1)\n",
    "\n",
    "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
    "        # inputs_tiled.shape=[None, num_capsule, input_num_capsule, input_dim_capsule]\n",
    "        inputs_tiled = K.tile(inputs_expand, [1, self.num_capsule, 1, 1])\n",
    "\n",
    "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0.\n",
    "        # x.shape=[num_capsule, input_num_capsule, input_dim_capsule]\n",
    "        # W.shape=[num_capsule, input_num_capsule, dim_capsule, input_dim_capsule]\n",
    "        # Regard the first two dimensions as `batch` dimension,\n",
    "        # then matmul: [input_dim_capsule] x [dim_capsule, input_dim_capsule]^T -> [dim_capsule].\n",
    "        # inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "        inputs_hat = K.map_fn(lambda x: own_batch_dot(x, self.W, [2, 3]), elems=inputs_tiled)\n",
    "\n",
    "        \"\"\"\n",
    "        # Begin: routing algorithm V1, dynamic ------------------------------------------------------------#\n",
    "        # The prior for coupling coefficient, initialized as zeros.\n",
    "        b = K.zeros(shape=[self.batch_size, self.num_capsule, self.input_num_capsule])\n",
    "        def body(i, b, outputs):\n",
    "            c = tf.nn.softmax(b, dim=1)  # dim=2 is the num_capsule dimension\n",
    "            outputs = squash(K.batch_dot(c, inputs_hat, [2, 2]))\n",
    "            if i != 1:\n",
    "                b = b + K.batch_dot(outputs, inputs_hat, [2, 3])\n",
    "            return [i-1, b, outputs]\n",
    "        cond = lambda i, b, inputs_hat: i > 0\n",
    "        loop_vars = [K.constant(self.num_routing), b, K.sum(inputs_hat, 2, keepdims=False)]\n",
    "        shape_invariants = [tf.TensorShape([]),\n",
    "                            tf.TensorShape([None, self.num_capsule, self.input_num_capsule]),\n",
    "                            tf.TensorShape([None, self.num_capsule, self.dim_capsule])]\n",
    "        _, _, outputs = tf.while_loop(cond, body, loop_vars, shape_invariants)\n",
    "        # End: routing algorithm V1, dynamic ------------------------------------------------------------#\n",
    "        \"\"\"\n",
    "        # Begin: Routing algorithm ---------------------------------------------------------------------#\n",
    "        # In forward pass, `inputs_hat_stopped` = `inputs_hat`;\n",
    "        # In backward, no gradient can flow from `inputs_hat_stopped` back to `inputs_hat`.\n",
    "        inputs_hat_stopped = K.stop_gradient(inputs_hat)\n",
    "        \n",
    "        # The prior for coupling coefficient, initialized as zeros.\n",
    "        # b.shape = [None, self.num_capsule, self.input_num_capsule].\n",
    "        b = tf.zeros(shape=(K.shape(inputs_hat)[0], self.num_capsule, self.input_num_capsule))\n",
    "\n",
    "        assert self.num_routing > 0, 'The num_routing should be > 0.'\n",
    "        for i in range(self.num_routing):\n",
    "            # c.shape=[batch_size, num_capsule, input_num_capsule]\n",
    "            c = tf.nn.softmax(b, axis=1)\n",
    "\n",
    "            # At last iteration, use `inputs_hat` to compute `outputs` in order to backpropagate gradient\n",
    "            if i == self.num_routing - 1:\n",
    "                # c.shape =  [batch_size, num_capsule, input_num_capsule]\n",
    "                # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
    "                # The first two dimensions as `batch` dimension,\n",
    "                # then matmal: [input_num_capsule] x [input_num_capsule, dim_capsule] -> [dim_capsule].\n",
    "                # outputs.shape=[None, num_capsule, dim_capsule]\n",
    "                outputs = squash(own_batch_dot(c, inputs_hat, [2, 2]))  # [None, 10, 16]\n",
    "            else:  # Otherwise, use `inputs_hat_stopped` to update `b`. No gradients flow on this path.\n",
    "                outputs = squash(own_batch_dot(c, inputs_hat_stopped, [2, 2]))\n",
    "\n",
    "                # outputs.shape =  [None, num_capsule, dim_capsule]\n",
    "                # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
    "                # The first two dimensions as `batch` dimension,\n",
    "                # then matmal: [dim_capsule] x [input_num_capsule, dim_capsule]^T -> [input_num_capsule].\n",
    "                # b.shape=[batch_size, num_capsule, input_num_capsule]\n",
    "                b += own_batch_dot(outputs, inputs_hat_stopped, [2, 3])\n",
    "        # End: Routing algorithm -----------------------------------------------------------------------#\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple([None, self.num_capsule, self.dim_capsule])\n",
    "\n",
    "\n",
    "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):\n",
    "    \"\"\"\n",
    "    Apply Conv2D `n_channels` times and concatenate all capsules\n",
    "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
    "    :param dim_capsule: the dim of the output vector of capsule\n",
    "    :param n_channels: the number of types of capsules\n",
    "    :return: output tensor, shape=[None, num_capsule, dim_capsule]\n",
    "    \"\"\"\n",
    "    output = layers.Conv1D(filters=dim_capsule*n_channels, kernel_size=kernel_size, strides=strides, padding=padding,\n",
    "                           name='primarycap_conv2d')(inputs)\n",
    "    outputs = layers.Reshape(target_shape=[-1, dim_capsule], name='primarycap_reshape')(output)\n",
    "    outputs = layers.Dropout(0.2)(outputs)\n",
    "    return layers.Lambda(squash, name='primarycap_squash')(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59834e56-d528-45c4-9d12-7243d4e7893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CapsNet(input_shape,top_words, maxlen, n_class, routings):\n",
    "    \"\"\"\n",
    "    :param input_shape: data shape, 3d, [width, height, channels]\n",
    "    :param n_class: number of classes\n",
    "    :param routings: number of routing iterations\n",
    "    :return: Two Keras Models, the first one used for training, and the second one for evaluation.\n",
    "            `eval_model` can also be used for training.\n",
    "    \"\"\"\n",
    "    x = layers.Input(shape=input_shape)\n",
    "\n",
    "\n",
    "    # Bi-LSTM layer\n",
    "    s= Bidirectional(LSTM(256,return_sequences=True))(x)\n",
    "   \n",
    "    # Layer 1: Just a conventional Conv1D layer\n",
    "    conv1 = layers.Conv1D(filters=256, kernel_size=12, strides=1, padding='valid', activation='relu', name='conv1')(s)\n",
    "\n",
    "    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_capsule]\n",
    "    primarycaps = PrimaryCap(conv1, dim_capsule=5, n_channels=10, kernel_size=13, strides=2, padding='valid')\n",
    "    \n",
    "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
    "    digitcaps = CapsuleLayer(num_capsule=n_class, dim_capsule=5, num_routing=routings,\n",
    "                             name='digitcaps')(primarycaps)\n",
    "    \n",
    "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
    "    out_caps = Length(name='capsnet')(digitcaps) \n",
    "\n",
    "     # Models for training and evaluation (prediction)\n",
    "    train_model = models.Model(x,out_caps) #masked_by_y\n",
    "    \n",
    "    return train_model\n",
    "\n",
    "def margin_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
    "    :param y_true: [None, n_classes]\n",
    "    :param y_pred: [None, num_capsule]\n",
    "    :return: a scalar loss value.\n",
    "    \"\"\"\n",
    "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
    "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
    "\n",
    "    return K.mean(K.sum(L, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fcdc371-bfd3-4b6e-b7c9-2be98f485b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    SEED = 42\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    \n",
    "    # the data, shuffled and split between train and test sets\n",
    "    TRAIN_SET = \"../data/ready_data/train_p2.csv\"\n",
    "    \n",
    "\n",
    "    # embedding and convolution parameters\n",
    "    MAX_SEQ_LENGTH = 1000\n",
    "    CONSIDERED_AA = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    VOCAB_SIZE = len(CONSIDERED_AA)\n",
    "  \n",
    "    # create train dataset\n",
    "    sequences_train, labels_train = create_dataset(data_path=TRAIN_SET, name=\"train_p2\", mode=\"train\")\n",
    "\n",
    "    # create test dataset\n",
    "    sequences_test, labels_test = create_dataset(data_path=TRAIN_SET, name=\"pdb2272\", mode=\"test\")\n",
    "\n",
    "    # encode sequences\n",
    "    sequences_train_encoded = np.concatenate(\n",
    "        [\n",
    "            one_hot_encoding(seq, MAX_SEQ_LENGTH, CONSIDERED_AA)\n",
    "            for seq in sequences_train\n",
    "        ],\n",
    "        axis=0,\n",
    "    )  # (14189, 1000, 20)\n",
    "    sequences_test_encoded = np.concatenate(\n",
    "        [\n",
    "            one_hot_encoding(seq, MAX_SEQ_LENGTH, CONSIDERED_AA)\n",
    "            for seq in sequences_test\n",
    "        ],\n",
    "        axis=0,\n",
    "    )  # (2272, 1000, 20)\n",
    "\n",
    "    # encode labels\n",
    "    labels_train_encoded = to_categorical(\n",
    "        labels_train, num_classes=2, dtype=\"float32\"\n",
    "    )  # (14189, 2)\n",
    "    labels_test_encoded = to_categorical(\n",
    "        labels_test, num_classes=2, dtype=\"float32\"\n",
    "    )  # (2272, 2)\n",
    "       \n",
    "   \n",
    "    return sequences_train_encoded, labels_train_encoded, sequences_test_encoded, labels_test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2cf19fe-a860-4ea3-901d-74801390daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "sp_per_fold = []\n",
    "prec_per_fold = []\n",
    "sn_per_fold = []\n",
    "AUC_per_fold = []\n",
    "MCC_per_fold = []\n",
    "epoch = 7\n",
    "\n",
    "# unpacking the data\n",
    "inputs, targets, X_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64862f01-e888-4ceb-882a-1e4ae6376967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 1000, 20)]        0         \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirecti  (None, 1000, 512)         567296    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " conv1 (Conv1D)              (None, 989, 256)          1573120   \n",
      "                                                                 \n",
      " primarycap_conv2d (Conv1D)  (None, 489, 50)           166450    \n",
      "                                                                 \n",
      " primarycap_reshape (Reshap  (None, 4890, 5)           0         \n",
      " e)                                                              \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 4890, 5)           0         \n",
      "                                                                 \n",
      " primarycap_squash (Lambda)  (None, 4890, 5)           0         \n",
      "                                                                 \n",
      " digitcaps (CapsuleLayer)    (None, 2, 5)              244500    \n",
      "                                                                 \n",
      " capsnet (Length)            (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2551366 (9.73 MB)\n",
      "Trainable params: 2551366 (9.73 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/7\n",
      "398/398 [==============================] - 125s 301ms/step - loss: 0.1821 - accuracy: 0.7173 - specificity: 0.4586 - precision: 0.6258 - recall: 0.9060 - auc: 0.7469 - matthews_correlation_coefficient: 0.3941 - lr: 0.0010\n",
      "Epoch 2/7\n",
      "398/398 [==============================] - 133s 335ms/step - loss: 0.1058 - accuracy: 0.8439 - specificity: 0.6787 - precision: 0.7442 - recall: 0.9347 - auc: 0.9123 - matthews_correlation_coefficient: 0.6356 - lr: 9.0000e-04\n",
      "Epoch 3/7\n",
      "398/398 [==============================] - 133s 335ms/step - loss: 0.0908 - accuracy: 0.8715 - specificity: 0.7274 - precision: 0.7760 - recall: 0.9455 - auc: 0.9362 - matthews_correlation_coefficient: 0.6902 - lr: 8.1000e-04\n",
      "Epoch 4/7\n",
      "398/398 [==============================] - 135s 340ms/step - loss: 0.0783 - accuracy: 0.8922 - specificity: 0.7701 - precision: 0.8060 - recall: 0.9551 - auc: 0.9524 - matthews_correlation_coefficient: 0.7385 - lr: 7.2900e-04\n",
      "Epoch 5/7\n",
      "398/398 [==============================] - 134s 337ms/step - loss: 0.0679 - accuracy: 0.9117 - specificity: 0.8036 - precision: 0.8305 - recall: 0.9633 - auc: 0.9647 - matthews_correlation_coefficient: 0.7774 - lr: 6.5610e-04\n",
      "Epoch 6/7\n",
      "398/398 [==============================] - 135s 340ms/step - loss: 0.0575 - accuracy: 0.9305 - specificity: 0.8392 - precision: 0.8581 - recall: 0.9725 - auc: 0.9751 - matthews_correlation_coefficient: 0.8193 - lr: 5.9049e-04\n",
      "Epoch 7/7\n",
      "398/398 [==============================] - 134s 336ms/step - loss: 0.0485 - accuracy: 0.9452 - specificity: 0.8681 - precision: 0.8811 - recall: 0.9794 - auc: 0.9828 - matthews_correlation_coefficient: 0.8531 - lr: 5.3144e-04\n",
      "Score for fold 1: loss of 0.07470047473907471; accuracy of 90.26903510093689%; specificity of 0.8536432385444641; precision of 0.8632729649543762; recall of 0.925818145275116; auc of 0.9576305150985718; matthews_correlation_coefficient of 0.7824779748916626\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 1000, 20)]        0         \n",
      "                                                                 \n",
      " bidirectional_6 (Bidirecti  (None, 1000, 512)         567296    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " conv1 (Conv1D)              (None, 989, 256)          1573120   \n",
      "                                                                 \n",
      " primarycap_conv2d (Conv1D)  (None, 489, 50)           166450    \n",
      "                                                                 \n",
      " primarycap_reshape (Reshap  (None, 4890, 5)           0         \n",
      " e)                                                              \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 4890, 5)           0         \n",
      "                                                                 \n",
      " primarycap_squash (Lambda)  (None, 4890, 5)           0         \n",
      "                                                                 \n",
      " digitcaps (CapsuleLayer)    (None, 2, 5)              244500    \n",
      "                                                                 \n",
      " capsnet (Length)            (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2551366 (9.73 MB)\n",
      "Trainable params: 2551366 (9.73 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/7\n",
      "398/398 [==============================] - 135s 331ms/step - loss: 0.1614 - accuracy: 0.7556 - specificity: 0.5160 - precision: 0.6518 - recall: 0.9064 - auc: 0.7992 - matthews_correlation_coefficient: 0.4511 - lr: 0.0010\n",
      "Epoch 2/7\n",
      "398/398 [==============================] - 134s 336ms/step - loss: 0.1064 - accuracy: 0.8455 - specificity: 0.6729 - precision: 0.7408 - recall: 0.9353 - auc: 0.9119 - matthews_correlation_coefficient: 0.6313 - lr: 9.0000e-04\n",
      "Epoch 3/7\n",
      "398/398 [==============================] - 134s 336ms/step - loss: 0.0886 - accuracy: 0.8768 - specificity: 0.7337 - precision: 0.7806 - recall: 0.9479 - auc: 0.9391 - matthews_correlation_coefficient: 0.6984 - lr: 8.1000e-04\n",
      "Epoch 4/7\n",
      "398/398 [==============================] - 132s 332ms/step - loss: 0.0738 - accuracy: 0.9029 - specificity: 0.7819 - precision: 0.8147 - recall: 0.9595 - auc: 0.9583 - matthews_correlation_coefficient: 0.7541 - lr: 7.2900e-04\n",
      "Epoch 5/7\n",
      "398/398 [==============================] - 132s 333ms/step - loss: 0.0625 - accuracy: 0.9222 - specificity: 0.8179 - precision: 0.8418 - recall: 0.9694 - auc: 0.9709 - matthews_correlation_coefficient: 0.7970 - lr: 6.5610e-04\n",
      "Epoch 6/7\n",
      "398/398 [==============================] - 135s 340ms/step - loss: 0.0509 - accuracy: 0.9425 - specificity: 0.8584 - precision: 0.8735 - recall: 0.9789 - auc: 0.9816 - matthews_correlation_coefficient: 0.8440 - lr: 5.9049e-04\n",
      "Epoch 7/7\n",
      "398/398 [==============================] - 136s 341ms/step - loss: 0.0413 - accuracy: 0.9598 - specificity: 0.8890 - precision: 0.8987 - recall: 0.9857 - auc: 0.9884 - matthews_correlation_coefficient: 0.8791 - lr: 5.3144e-04\n",
      "Score for fold 2: loss of 0.07607419788837433; accuracy of 91.04782938957214%; specificity of 0.8525439500808716; precision of 0.8627364635467529; recall of 0.9255821108818054; auc of 0.9549779891967773; matthews_correlation_coefficient of 0.7806753516197205\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 1000, 20)]        0         \n",
      "                                                                 \n",
      " bidirectional_7 (Bidirecti  (None, 1000, 512)         567296    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " conv1 (Conv1D)              (None, 989, 256)          1573120   \n",
      "                                                                 \n",
      " primarycap_conv2d (Conv1D)  (None, 489, 50)           166450    \n",
      "                                                                 \n",
      " primarycap_reshape (Reshap  (None, 4890, 5)           0         \n",
      " e)                                                              \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 4890, 5)           0         \n",
      "                                                                 \n",
      " primarycap_squash (Lambda)  (None, 4890, 5)           0         \n",
      "                                                                 \n",
      " digitcaps (CapsuleLayer)    (None, 2, 5)              244500    \n",
      "                                                                 \n",
      " capsnet (Length)            (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2551366 (9.73 MB)\n",
      "Trainable params: 2551366 (9.73 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/7\n",
      "398/398 [==============================] - 139s 340ms/step - loss: 0.1521 - accuracy: 0.7655 - specificity: 0.5408 - precision: 0.6631 - recall: 0.9047 - auc: 0.8241 - matthews_correlation_coefficient: 0.4767 - lr: 0.0010\n",
      "Epoch 2/7\n",
      "398/398 [==============================] - 135s 339ms/step - loss: 0.1027 - accuracy: 0.8510 - specificity: 0.6851 - precision: 0.7487 - recall: 0.9384 - auc: 0.9186 - matthews_correlation_coefficient: 0.6456 - lr: 9.0000e-04\n",
      "Epoch 3/7\n",
      "398/398 [==============================] - 133s 335ms/step - loss: 0.0851 - accuracy: 0.8824 - specificity: 0.7454 - precision: 0.7890 - recall: 0.9521 - auc: 0.9448 - matthews_correlation_coefficient: 0.7135 - lr: 8.1000e-04\n",
      "Epoch 4/7\n",
      "398/398 [==============================] - 131s 330ms/step - loss: 0.0719 - accuracy: 0.9066 - specificity: 0.7912 - precision: 0.8215 - recall: 0.9617 - auc: 0.9612 - matthews_correlation_coefficient: 0.7647 - lr: 7.2900e-04\n",
      "Epoch 5/7\n",
      "398/398 [==============================] - 137s 344ms/step - loss: 0.0573 - accuracy: 0.9319 - specificity: 0.8399 - precision: 0.8587 - recall: 0.9737 - auc: 0.9763 - matthews_correlation_coefficient: 0.8215 - lr: 6.5610e-04\n",
      "Epoch 6/7\n",
      "398/398 [==============================] - 138s 347ms/step - loss: 0.0469 - accuracy: 0.9499 - specificity: 0.8733 - precision: 0.8855 - recall: 0.9809 - auc: 0.9848 - matthews_correlation_coefficient: 0.8596 - lr: 5.9049e-04\n",
      "Epoch 7/7\n",
      "398/398 [==============================] - 135s 340ms/step - loss: 0.0374 - accuracy: 0.9657 - specificity: 0.9033 - precision: 0.9110 - recall: 0.9882 - auc: 0.9908 - matthews_correlation_coefficient: 0.8951 - lr: 5.3144e-04\n",
      "Score for fold 3: loss of 0.07083061337471008; accuracy of 91.66929125785828%; specificity of 0.8708385825157166; precision of 0.8778364658355713; recall of 0.928178071975708; auc of 0.9621731042861938; matthews_correlation_coefficient of 0.8011479377746582\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 1000, 20)]        0         \n",
      "                                                                 \n",
      " bidirectional_8 (Bidirecti  (None, 1000, 512)         567296    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " conv1 (Conv1D)              (None, 989, 256)          1573120   \n",
      "                                                                 \n",
      " primarycap_conv2d (Conv1D)  (None, 489, 50)           166450    \n",
      "                                                                 \n",
      " primarycap_reshape (Reshap  (None, 4890, 5)           0         \n",
      " e)                                                              \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 4890, 5)           0         \n",
      "                                                                 \n",
      " primarycap_squash (Lambda)  (None, 4890, 5)           0         \n",
      "                                                                 \n",
      " digitcaps (CapsuleLayer)    (None, 2, 5)              244500    \n",
      "                                                                 \n",
      " capsnet (Length)            (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2551366 (9.73 MB)\n",
      "Trainable params: 2551366 (9.73 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/7\n",
      "398/398 [==============================] - 142s 347ms/step - loss: 0.1916 - accuracy: 0.7083 - specificity: 0.4263 - precision: 0.6142 - recall: 0.9136 - auc: 0.7264 - matthews_correlation_coefficient: 0.3701 - lr: 0.0010\n",
      "Epoch 2/7\n",
      "398/398 [==============================] - 131s 328ms/step - loss: 0.1084 - accuracy: 0.8427 - specificity: 0.6682 - precision: 0.7377 - recall: 0.9334 - auc: 0.9085 - matthews_correlation_coefficient: 0.6248 - lr: 9.0000e-04\n",
      "Epoch 3/7\n",
      "398/398 [==============================] - 134s 337ms/step - loss: 0.0903 - accuracy: 0.8735 - specificity: 0.7300 - precision: 0.7781 - recall: 0.9465 - auc: 0.9371 - matthews_correlation_coefficient: 0.6935 - lr: 8.1000e-04\n",
      "Epoch 4/7\n",
      "398/398 [==============================] - 133s 333ms/step - loss: 0.0775 - accuracy: 0.8964 - specificity: 0.7717 - precision: 0.8074 - recall: 0.9567 - auc: 0.9541 - matthews_correlation_coefficient: 0.7417 - lr: 7.2900e-04\n",
      "Epoch 5/7\n",
      "398/398 [==============================] - 136s 343ms/step - loss: 0.0663 - accuracy: 0.9156 - specificity: 0.8086 - precision: 0.8346 - recall: 0.9664 - auc: 0.9673 - matthews_correlation_coefficient: 0.7854 - lr: 6.5610e-04\n",
      "Epoch 6/7\n",
      "398/398 [==============================] - 138s 346ms/step - loss: 0.0554 - accuracy: 0.9337 - specificity: 0.8445 - precision: 0.8626 - recall: 0.9737 - auc: 0.9777 - matthews_correlation_coefficient: 0.8254 - lr: 5.9049e-04\n",
      "Epoch 7/7\n",
      "398/398 [==============================] - 138s 346ms/step - loss: 0.0459 - accuracy: 0.9498 - specificity: 0.8762 - precision: 0.8881 - recall: 0.9813 - auc: 0.9851 - matthews_correlation_coefficient: 0.8627 - lr: 5.3144e-04\n",
      "Score for fold 4: loss of 0.08337816596031189; accuracy of 89.54448699951172%; specificity of 0.8154836893081665; precision of 0.8322985768318176; recall of 0.9175517559051514; auc of 0.9471482038497925; matthews_correlation_coefficient of 0.737872838973999\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 1000, 20)]        0         \n",
      "                                                                 \n",
      " bidirectional_9 (Bidirecti  (None, 1000, 512)         567296    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " conv1 (Conv1D)              (None, 989, 256)          1573120   \n",
      "                                                                 \n",
      " primarycap_conv2d (Conv1D)  (None, 489, 50)           166450    \n",
      "                                                                 \n",
      " primarycap_reshape (Reshap  (None, 4890, 5)           0         \n",
      " e)                                                              \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 4890, 5)           0         \n",
      "                                                                 \n",
      " primarycap_squash (Lambda)  (None, 4890, 5)           0         \n",
      "                                                                 \n",
      " digitcaps (CapsuleLayer)    (None, 2, 5)              244500    \n",
      "                                                                 \n",
      " capsnet (Length)            (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2551366 (9.73 MB)\n",
      "Trainable params: 2551366 (9.73 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/7\n",
      "398/398 [==============================] - 141s 347ms/step - loss: 0.1479 - accuracy: 0.7733 - specificity: 0.5406 - precision: 0.6645 - recall: 0.9101 - auc: 0.8333 - matthews_correlation_coefficient: 0.4818 - lr: 0.0010\n",
      "Epoch 2/7\n",
      "398/398 [==============================] - 132s 333ms/step - loss: 0.0997 - accuracy: 0.8578 - specificity: 0.6965 - precision: 0.7561 - recall: 0.9413 - auc: 0.9237 - matthews_correlation_coefficient: 0.6587 - lr: 9.0000e-04\n",
      "Epoch 3/7\n",
      "398/398 [==============================] - 134s 337ms/step - loss: 0.0827 - accuracy: 0.8879 - specificity: 0.7548 - precision: 0.7955 - recall: 0.9538 - auc: 0.9483 - matthews_correlation_coefficient: 0.7239 - lr: 8.1000e-04\n",
      "Epoch 4/7\n",
      "398/398 [==============================] - 132s 331ms/step - loss: 0.0683 - accuracy: 0.9133 - specificity: 0.8040 - precision: 0.8311 - recall: 0.9635 - auc: 0.9650 - matthews_correlation_coefficient: 0.7779 - lr: 7.2900e-04\n",
      "Epoch 5/7\n",
      "398/398 [==============================] - 136s 341ms/step - loss: 0.0544 - accuracy: 0.9357 - specificity: 0.8493 - precision: 0.8661 - recall: 0.9750 - auc: 0.9783 - matthews_correlation_coefficient: 0.8313 - lr: 6.5610e-04\n",
      "Epoch 6/7\n",
      "398/398 [==============================] - 139s 350ms/step - loss: 0.0485 - accuracy: 0.9481 - specificity: 0.8694 - precision: 0.8826 - recall: 0.9809 - auc: 0.9821 - matthews_correlation_coefficient: 0.8561 - lr: 5.9049e-04\n",
      "Epoch 7/7\n",
      "398/398 [==============================] - 138s 347ms/step - loss: 0.0370 - accuracy: 0.9659 - specificity: 0.9054 - precision: 0.9128 - recall: 0.9891 - auc: 0.9906 - matthews_correlation_coefficient: 0.8980 - lr: 5.3144e-04\n",
      "Score for fold 5: loss of 0.07391401380300522; accuracy of 90.70883393287659%; specificity of 0.8164258599281311; precision of 0.8362515568733215; recall of 0.9393438696861267; auc of 0.9553049206733704; matthews_correlation_coefficient of 0.7628918886184692\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.07470047473907471 - Accuracy: 90.26903510093689% - Specificity: 0.8536432385444641 - Precision: 0.8632729649543762 - Sensitivity: 0.925818145275116 - AUC: 0.9576305150985718 - MCC: 0.7824779748916626\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.07607419788837433 - Accuracy: 91.04782938957214% - Specificity: 0.8525439500808716 - Precision: 0.8627364635467529 - Sensitivity: 0.9255821108818054 - AUC: 0.9549779891967773 - MCC: 0.7806753516197205\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.07083061337471008 - Accuracy: 91.66929125785828% - Specificity: 0.8708385825157166 - Precision: 0.8778364658355713 - Sensitivity: 0.928178071975708 - AUC: 0.9621731042861938 - MCC: 0.8011479377746582\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.08337816596031189 - Accuracy: 89.54448699951172% - Specificity: 0.8154836893081665 - Precision: 0.8322985768318176 - Sensitivity: 0.9175517559051514 - AUC: 0.9471482038497925 - MCC: 0.737872838973999\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.07391401380300522 - Accuracy: 90.70883393287659% - Specificity: 0.8164258599281311 - Precision: 0.8362515568733215 - Sensitivity: 0.9393438696861267 - AUC: 0.9553049206733704 - MCC: 0.7628918886184692\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 90.64789533615112 (+- 0.7166526946476324)\n",
      "> Loss: 0.07577949315309525\n",
      "> Specificity: 0.84178706407547\n",
      "> Precision: 0.8544792056083679\n",
      "> Sensitivity: 0.9272947907447815\n",
      "> AUC: 0.9554469466209412\n",
      "> MCC: 0.7730131983757019\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# embedding and convolution parameters\n",
    "MAX_SEQ_LENGTH = 1000\n",
    "CONSIDERED_AA = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "VOCAB_SIZE = len(CONSIDERED_AA)\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "models_valid = []\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "\n",
    "  model = CapsNet((1000, 20), VOCAB_SIZE, MAX_SEQ_LENGTH, n_class=2, routings=2)\n",
    "  model.summary()\n",
    "\n",
    "  lr_decay = LearningRateScheduler(schedule=lambda epoch: 0.001 * (0.9** epoch))\n",
    "# compile the model\n",
    "  model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001),\n",
    "                  loss=[margin_loss, 'binary_crossentropy'],\n",
    "                  metrics=[\"accuracy\", specificity, \"Precision\", \"Recall\", \"AUC\", matthews_correlation_coefficient])\n",
    "\n",
    "  # Generate a print\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "  history = model.fit(inputs[train], targets[train],\n",
    "              batch_size=128,\n",
    "              epochs=epoch,\n",
    "              verbose=1,\n",
    "              callbacks=[lr_decay])\n",
    "    \n",
    "  # Generate generalization metrics\n",
    "  scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%; {model.metrics_names[2]} of {scores[2]}; {model.metrics_names[3]} of {scores[3]}; {model.metrics_names[4]} of {scores[4]}; {model.metrics_names[5]} of {scores[5]}; {model.metrics_names[6]} of {scores[6]}')\n",
    "  acc_per_fold.append(scores[1] * 100)\n",
    "  loss_per_fold.append(scores[0])\n",
    "  sp_per_fold.append(scores[2])\n",
    "  prec_per_fold.append(scores[3])\n",
    "  sn_per_fold.append(scores[4])\n",
    "  AUC_per_fold.append(scores[5])\n",
    "  MCC_per_fold.append(scores[6])\n",
    "\n",
    "  # Increase fold number\n",
    "  fold_no = fold_no + 1\n",
    "  models_valid.append(model)\n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}% - Specificity: {sp_per_fold[i]} - Precision: {prec_per_fold[i]} - Sensitivity: {sn_per_fold[i]} - AUC: {AUC_per_fold[i]} - MCC: {MCC_per_fold[i]}')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print(f'> Specificity: {np.mean(sp_per_fold)}')\n",
    "print(f'> Precision: {np.mean(prec_per_fold)}')\n",
    "print(f'> Sensitivity: {np.mean(sn_per_fold)}')\n",
    "print(f'> AUC: {np.mean(AUC_per_fold)}')\n",
    "print(f'> MCC: {np.mean(MCC_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fcfe52c9-ef3a-4d39-8158-ec2e330dcef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Accuracy: 71.13556265830994\n",
      "> Specificity: 0.6213908553123474\n",
      "> Precision: 0.6664837837219239\n",
      "> Sensitivity: 0.755897867679596\n",
      "> AUC: 0.7607316374778748\n",
      "> MCC: 0.3818281352519989\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "sp = []\n",
    "prec = []\n",
    "sn = []\n",
    "AUC = []\n",
    "MCC = []\n",
    "\n",
    "\n",
    "for model in models_valid:\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    acc.append(scores[1] * 100)\n",
    "    sp.append(scores[2])\n",
    "    prec.append(scores[3])\n",
    "    sn.append(scores[4])\n",
    "    AUC.append(scores[5])\n",
    "    MCC.append(scores[6])\n",
    "\n",
    "print(f'> Accuracy: {np.mean(acc)}')\n",
    "print(f'> Specificity: {np.mean(sp)}')\n",
    "print(f'> Precision: {np.mean(prec)}')\n",
    "print(f'> Sensitivity: {np.mean(sn)}')\n",
    "print(f'> AUC: {np.mean(AUC)}')\n",
    "print(f'> MCC: {np.mean(MCC)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb5b4ca-0ec6-4443-91d7-afe8663eea62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
