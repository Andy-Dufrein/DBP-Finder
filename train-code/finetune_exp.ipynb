{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ankh\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import BatchSampler, DataLoader, Dataset, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clearml\n",
    "from clearml import Logger, Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=9b912b2281484adca0cba97e26b1a38d\n",
      "2024-06-20 14:05:45,131 - clearml.Task - INFO - Storing jupyter notebook directly as code\n",
      "ClearML results page: https://app.clear.ml/projects/45ba7ff7a93646a8a76d1950065cf1d5/experiments/9b912b2281484adca0cba97e26b1a38d/output/log\n",
      "ClearML Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start\n"
     ]
    }
   ],
   "source": [
    "clearml.browser_login()\n",
    "task = Task.init(\n",
    "    project_name=\"DBPs_search\",\n",
    "    task_name=\"Finetune Ankh v3\",\n",
    "    output_uri=True,\n",
    ")\n",
    "logger = Logger.current_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_config': {'input_dim': 1536,\n",
       "  'nhead': 4,\n",
       "  'hidden_dim': 1536,\n",
       "  'num_hidden_layers': 1,\n",
       "  'num_layers': 1,\n",
       "  'kernel_size': 7,\n",
       "  'dropout': 0.2,\n",
       "  'pooling': 'max'},\n",
       " 'training_config': {'epochs': 10,\n",
       "  'lr': '2e-4',\n",
       "  'seed': 42,\n",
       "  'factor': 0.5,\n",
       "  'patience': 2,\n",
       "  'min_lr': '1e-6',\n",
       "  'batch_size': 64,\n",
       "  'num_workers': 4,\n",
       "  'optimizer': 'adamw'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.connect_configuration(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = config[\"model_config\"][\"input_dim\"]\n",
    "nhead = config[\"model_config\"][\"nhead\"]\n",
    "hidden_dim = config[\"model_config\"][\"hidden_dim\"]\n",
    "num_hidden_layers = config[\"model_config\"][\"num_hidden_layers\"]\n",
    "num_layers = config[\"model_config\"][\"num_layers\"]\n",
    "kernel_size = config[\"model_config\"][\"kernel_size\"]\n",
    "dropout = config[\"model_config\"][\"dropout\"]\n",
    "pooling = config[\"model_config\"][\"pooling\"]\n",
    "\n",
    "\n",
    "epochs = config[\"training_config\"][\"epochs\"]\n",
    "lr = config[\"training_config\"][\"lr\"]\n",
    "factor = config[\"training_config\"][\"factor\"]\n",
    "patience = config[\"training_config\"][\"patience\"]\n",
    "min_lr = config[\"training_config\"][\"min_lr\"]\n",
    "batch_size = config[\"training_config\"][\"batch_size\"]\n",
    "seed = config[\"training_config\"][\"seed\"]\n",
    "num_workers = config[\"training_config\"][\"num_workers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict_from_hdf5(filename):\n",
    "    \"\"\"\n",
    "    Load a dictionary with string keys and NumPy array values from an HDF5 file.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): Name of the HDF5 file to load the data from.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary with string keys and NumPy array values.\n",
    "    \"\"\"\n",
    "    loaded_dict = {}\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        for key in f.keys():\n",
    "            loaded_dict[key] = f[key][:]\n",
    "    return loaded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(\n",
    "    all_labels: list, all_preds: list, logits: list\n",
    ") -> dict[str, float]:\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    logits = np.array(logits)\n",
    "\n",
    "    auc = roc_auc_score(all_labels, logits)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    specificity = recall_score(all_labels, all_preds, pos_label=0)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Sensitivity\": recall,\n",
    "        \"Specificity\": specificity,\n",
    "        \"Precision\": precision,\n",
    "        \"AUC\": auc,\n",
    "        \"F1\": f1,\n",
    "        \"MCC\": mcc,\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = load_dict_from_hdf5(\n",
    "    \"../../../../ssd2/dbp_finder/ankh_embeddings/train_p2_2d.h5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in embeddings:\n",
    "    embeddings[key] = np.squeeze(embeddings[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_df = pd.DataFrame(list(embeddings.items()), columns=[\"identifier\", \"embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/ready_data/train_pdb2272.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(embed_df, on=\"identifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    31803\n",
       "0    31803\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "368"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.embedding.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df[\"sequence\"].tolist()\n",
    "y = train_df[\"label\"].tolist()\n",
    "groups = train_df[\"cluster\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gkf = GroupKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folds = []\n",
    "valid_folds = []\n",
    "\n",
    "for train_idx, valid_idx in gkf.split(X, y, groups=groups):\n",
    "    train_idx = train_idx.tolist()\n",
    "    valid_idx = valid_idx.tolist()\n",
    "\n",
    "    train = train_df.iloc[train_idx]\n",
    "    valid = train_df.iloc[valid_idx]\n",
    "\n",
    "    train_folds.append(train)\n",
    "    valid_folds.append(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>sequence</th>\n",
       "      <th>label</th>\n",
       "      <th>cluster</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A096MJY4</td>\n",
       "      <td>MGRKKIQITRIMDERNRQVTFTKRKFGLMKKAYELSVLCDCEIALI...</td>\n",
       "      <td>1</td>\n",
       "      <td>1984</td>\n",
       "      <td>[[0.011778823, 0.004910938, 0.007578383, 0.011...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A0D2UG83</td>\n",
       "      <td>MAKSKKIVAATSGSRSRSSRAGLAFPVGRVHRLLRKGHFADRIGSG...</td>\n",
       "      <td>1</td>\n",
       "      <td>23507</td>\n",
       "      <td>[[0.029099701, -0.014861202, 0.019306818, 0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A0G2JTZ2</td>\n",
       "      <td>MSSKQATSPFACTVDGEETMTQDLTSREKEEGSDQHPASHLPLHPI...</td>\n",
       "      <td>1</td>\n",
       "      <td>11680</td>\n",
       "      <td>[[0.018502971, 0.0045635537, -0.006065971, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0A0G2L7I0</td>\n",
       "      <td>MMEDEDFLLALRLQEQFDQETPAAGWPDEDCPSSKRRRVDPSGGLD...</td>\n",
       "      <td>1</td>\n",
       "      <td>14394</td>\n",
       "      <td>[[0.027536467, -0.016558107, -0.0020665708, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A0A0G2Q9D6</td>\n",
       "      <td>MGKNEARRSALAPDHGTVVCDPLRRLNRMHATPEESIRIVAAQKKK...</td>\n",
       "      <td>1</td>\n",
       "      <td>23923</td>\n",
       "      <td>[[0.016459255, -0.0053423513, -0.0066505796, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63601</th>\n",
       "      <td>Q8WYP3</td>\n",
       "      <td>MTAWTMGARGLDKRGSFFKLIDTIASEIGELKQEMVRTDVNLENGL...</td>\n",
       "      <td>0</td>\n",
       "      <td>575</td>\n",
       "      <td>[[0.0080460785, -0.017565874, -0.015078276, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63602</th>\n",
       "      <td>O35550</td>\n",
       "      <td>MAQPGPAPQPDVSLQQRVAELEKINAEFLRAQQQLEQEFNQKRAKF...</td>\n",
       "      <td>0</td>\n",
       "      <td>27336</td>\n",
       "      <td>[[0.016553048, -0.023833824, -0.0103855645, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63603</th>\n",
       "      <td>Q9Z0W5</td>\n",
       "      <td>MSGPYDEASEEITDSFWEVGNYKRTVKRIDDGHRLCNDLMSCVQER...</td>\n",
       "      <td>0</td>\n",
       "      <td>3049</td>\n",
       "      <td>[[0.01263333, 0.0013439438, 0.012199068, 0.010...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63604</th>\n",
       "      <td>Q7TQ32</td>\n",
       "      <td>MGQSPSPRSPHGSPPTLSTLTLLLLLCGQAHSQCKILRCNAEYVSS...</td>\n",
       "      <td>0</td>\n",
       "      <td>18137</td>\n",
       "      <td>[[0.0138585605, -0.016503282, -0.004226524, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63605</th>\n",
       "      <td>Q9SD71</td>\n",
       "      <td>MEKSQKQVTRPSNSRREYSKEIPIDLLIEIFSRLSTGDIARCRCVS...</td>\n",
       "      <td>0</td>\n",
       "      <td>4109</td>\n",
       "      <td>[[0.028412536, -0.0093330825, 0.015916236, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50884 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       identifier                                           sequence  label  \\\n",
       "0      A0A096MJY4  MGRKKIQITRIMDERNRQVTFTKRKFGLMKKAYELSVLCDCEIALI...      1   \n",
       "1      A0A0D2UG83  MAKSKKIVAATSGSRSRSSRAGLAFPVGRVHRLLRKGHFADRIGSG...      1   \n",
       "2      A0A0G2JTZ2  MSSKQATSPFACTVDGEETMTQDLTSREKEEGSDQHPASHLPLHPI...      1   \n",
       "4      A0A0G2L7I0  MMEDEDFLLALRLQEQFDQETPAAGWPDEDCPSSKRRRVDPSGGLD...      1   \n",
       "5      A0A0G2Q9D6  MGKNEARRSALAPDHGTVVCDPLRRLNRMHATPEESIRIVAAQKKK...      1   \n",
       "...           ...                                                ...    ...   \n",
       "63601      Q8WYP3  MTAWTMGARGLDKRGSFFKLIDTIASEIGELKQEMVRTDVNLENGL...      0   \n",
       "63602      O35550  MAQPGPAPQPDVSLQQRVAELEKINAEFLRAQQQLEQEFNQKRAKF...      0   \n",
       "63603      Q9Z0W5  MSGPYDEASEEITDSFWEVGNYKRTVKRIDDGHRLCNDLMSCVQER...      0   \n",
       "63604      Q7TQ32  MGQSPSPRSPHGSPPTLSTLTLLLLLCGQAHSQCKILRCNAEYVSS...      0   \n",
       "63605      Q9SD71  MEKSQKQVTRPSNSRREYSKEIPIDLLIEIFSRLSTGDIARCRCVS...      0   \n",
       "\n",
       "       cluster                                          embedding  \n",
       "0         1984  [[0.011778823, 0.004910938, 0.007578383, 0.011...  \n",
       "1        23507  [[0.029099701, -0.014861202, 0.019306818, 0.02...  \n",
       "2        11680  [[0.018502971, 0.0045635537, -0.006065971, 0.0...  \n",
       "4        14394  [[0.027536467, -0.016558107, -0.0020665708, 0....  \n",
       "5        23923  [[0.016459255, -0.0053423513, -0.0066505796, 0...  \n",
       "...        ...                                                ...  \n",
       "63601      575  [[0.0080460785, -0.017565874, -0.015078276, 0....  \n",
       "63602    27336  [[0.016553048, -0.023833824, -0.0103855645, 0....  \n",
       "63603     3049  [[0.01263333, 0.0013439438, 0.012199068, 0.010...  \n",
       "63604    18137  [[0.0138585605, -0.016503282, -0.004226524, 0....  \n",
       "63605     4109  [[0.028412536, -0.0093330825, 0.015916236, 0.0...  \n",
       "\n",
       "[50884 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_folds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.embeds = df.embedding.tolist()\n",
    "        self.labels = df.label.tolist()\n",
    "        self.lengths = [len(embed) for embed in self.embeds]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeds)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.embeds[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        y = torch.tensor(y, dtype=torch.float)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    # Extract the embeddings from the batch\n",
    "    embeddings = [item[0] for item in batch]\n",
    "    labels = torch.tensor([item[1] for item in batch], dtype=torch.float)\n",
    "\n",
    "    # Pad the embeddings\n",
    "    padded_embeddings = pad_sequence(embeddings, batch_first=True)\n",
    "    return padded_embeddings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBatchSampler(BatchSampler):\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.sampler = SequentialSampler(dataset)\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = list(self.sampler)\n",
    "        indices.sort(\n",
    "            key=lambda i: self.dataset.lengths[i], reverse=True\n",
    "        )  # Sort indices by sequence length\n",
    "        batches = [\n",
    "            indices[i : i + self.batch_size]\n",
    "            for i in range(0, len(indices), self.batch_size)\n",
    "        ]\n",
    "        for batch in batches:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train - pd dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = SequenceDataset(train)\n",
    "# train_sampler = CustomBatchSampler(train_dataset, batch_size)\n",
    "# train_dataloader = DataLoader(\n",
    "#     train_dataset,\n",
    "#     num_workers=num_workers,\n",
    "#     batch_sampler=train_sampler,\n",
    "#     collate_fn=custom_collate_fn,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_dataset = SequenceDataset(valid)\n",
    "# valid_sampler = CustomBatchSampler(valid_dataset, batch_size)\n",
    "# valid_dataloader = DataLoader(\n",
    "#     valid_dataset,\n",
    "#     num_workers=num_workers,\n",
    "#     batch_sampler=valid_sampler,\n",
    "#     collate_fn=custom_collate_fn,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_classification_model = ankh.ConvBertForBinaryClassification(\n",
    "    input_dim=input_dim,\n",
    "    nhead=nhead,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_hidden_layers=num_hidden_layers,\n",
    "    num_layers=num_layers,\n",
    "    kernel_size=kernel_size,\n",
    "    dropout=dropout,\n",
    "    pooling=pooling,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_classification_model = binary_classification_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a, b = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = binary_classification_model(a.to(DEVICE), b.to(DEVICE).unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = binary_classification_model(a.to(DEVICE), b.to(DEVICE).unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = AdamW(binary_classification_model.parameters(), lr=float(lr))\n",
    "# scheduler = ReduceLROnPlateau(\n",
    "#     optimizer, mode=\"min\", factor=factor, patience=patience, min_lr=float(min_lr)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(binary_classification_model, train_dataloader, optimizer):\n",
    "    binary_classification_model.train()\n",
    "    loss = 0.0\n",
    "    for x, y in train_dataloader:\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "\n",
    "        y = y.unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = binary_classification_model(x, y)\n",
    "        output.loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss += output.loss.item()\n",
    "\n",
    "    epoch_loss = loss / len(train_dataloader)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_fn(binary_classification_model, test_dataloader, scheduler):\n",
    "    binary_classification_model.eval()\n",
    "    loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    logits = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_dataloader:\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "\n",
    "            y = y.unsqueeze(1)\n",
    "\n",
    "            output = binary_classification_model(x, y)\n",
    "            loss += output.loss.item()\n",
    "\n",
    "            preds = (output.logits > 0.5).float()\n",
    "\n",
    "            logits.extend(output.logits.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    epoch_loss = loss / len(test_dataloader)\n",
    "    scheduler.step(epoch_loss)\n",
    "    metrics = calculate_metrics(all_labels, all_preds, logits)\n",
    "    return epoch_loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-20 14:15:22,329 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_0.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 14:15:22,336 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 14:19:39,146 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_0.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 14:19:39,154 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 14:27:31,679 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_0.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 14:27:31,687 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 14:42:22,558 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_0.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 14:42:22,564 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 14:51:35,174 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_0.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 14:51:35,197 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 15:00:05,229 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_1.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 15:00:05,236 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 15:07:19,512 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_1.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 15:07:19,518 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 15:10:42,779 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_1.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 15:10:42,784 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 15:34:08,872 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_2.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 15:34:08,879 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 15:37:30,184 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_2.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 15:37:30,190 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 15:50:48,926 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_2.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 15:50:48,932 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 15:54:24,533 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_2.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 15:54:24,538 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 16:07:49,668 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_3.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 16:07:49,673 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 16:11:20,799 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_3.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 16:11:20,804 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 16:14:53,645 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_3.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 16:14:53,651 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 16:37:58,138 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_3.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 16:37:58,144 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 16:41:27,304 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_4.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 16:41:27,310 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 16:45:03,568 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_4.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 16:45:03,574 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 16:55:09,013 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_4.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 16:55:09,019 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 17:08:32,303 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_4.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 17:08:32,310 - clearml.Task - INFO - Failed model upload\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "for i in range(len(train_folds)):\n",
    "    train_dataset = SequenceDataset(train_folds[i])\n",
    "    train_sampler = CustomBatchSampler(train_dataset, batch_size)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        num_workers=num_workers,\n",
    "        batch_sampler=train_sampler,\n",
    "        collate_fn=custom_collate_fn,\n",
    "    )\n",
    "\n",
    "    valid_dataset = SequenceDataset(valid_folds[i])\n",
    "    valid_sampler = CustomBatchSampler(valid_dataset, batch_size)\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        num_workers=num_workers,\n",
    "        batch_sampler=valid_sampler,\n",
    "        collate_fn=custom_collate_fn,\n",
    "    )\n",
    "\n",
    "    binary_classification_model = ankh.ConvBertForBinaryClassification(\n",
    "        input_dim=input_dim,\n",
    "        nhead=nhead,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_hidden_layers=num_hidden_layers,\n",
    "        num_layers=num_layers,\n",
    "        kernel_size=kernel_size,\n",
    "        dropout=dropout,\n",
    "        pooling=pooling,\n",
    "    )\n",
    "\n",
    "    binary_classification_model = binary_classification_model.to(DEVICE)\n",
    "    optimizer = AdamW(binary_classification_model.parameters(), lr=float(lr))\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", factor=factor, patience=patience, min_lr=float(min_lr)\n",
    "    )\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model_path = f\"checkpoints/best_model_{i}.pth\"\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_fn(binary_classification_model, train_dataloader, optimizer)\n",
    "        valid_loss, metrics = validate_fn(\n",
    "            binary_classification_model, valid_dataloader, scheduler\n",
    "        )\n",
    "\n",
    "        logger.report_scalar(\n",
    "            title=f\"Loss model {i}\", series=\"train loss\", value=train_loss, iteration=epoch\n",
    "        )\n",
    "        logger.report_scalar(\n",
    "            title=f\"Loss model {i}\", series=\"valid loss\", value=valid_loss, iteration=epoch\n",
    "        )\n",
    "\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            logger.report_scalar(\n",
    "                title=f\"Metrics model {i}\", series=metric_name, value=metric_value, iteration=epoch\n",
    "            )\n",
    "\n",
    "        if valid_loss < best_val_loss:\n",
    "            best_val_loss = valid_loss\n",
    "            torch.save(binary_classification_model.state_dict(), best_model_path)\n",
    "            message = f'Saved Best Model on epoch {epoch} with Validation Loss: {best_val_loss}'\n",
    "            logger.report_text(message, level=logging.DEBUG, print_console=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference average best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = ankh.ConvBertForBinaryClassification(\n",
    "    input_dim=input_dim,\n",
    "    nhead=nhead,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_hidden_layers=num_hidden_layers,\n",
    "    num_layers=num_layers,\n",
    "    kernel_size=kernel_size,\n",
    "    dropout=dropout,\n",
    "    pooling=pooling,\n",
    ")\n",
    "\n",
    "model_1 = ankh.ConvBertForBinaryClassification(\n",
    "    input_dim=input_dim,\n",
    "    nhead=nhead,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_hidden_layers=num_hidden_layers,\n",
    "    num_layers=num_layers,\n",
    "    kernel_size=kernel_size,\n",
    "    dropout=dropout,\n",
    "    pooling=pooling,\n",
    ")\n",
    "\n",
    "model_2 = ankh.ConvBertForBinaryClassification(\n",
    "    input_dim=input_dim,\n",
    "    nhead=nhead,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_hidden_layers=num_hidden_layers,\n",
    "    num_layers=num_layers,\n",
    "    kernel_size=kernel_size,\n",
    "    dropout=dropout,\n",
    "    pooling=pooling,\n",
    ")\n",
    "\n",
    "model_3 = ankh.ConvBertForBinaryClassification(\n",
    "    input_dim=input_dim,\n",
    "    nhead=nhead,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_hidden_layers=num_hidden_layers,\n",
    "    num_layers=num_layers,\n",
    "    kernel_size=kernel_size,\n",
    "    dropout=dropout,\n",
    "    pooling=pooling,\n",
    ")\n",
    "\n",
    "model_4 = ankh.ConvBertForBinaryClassification(\n",
    "    input_dim=input_dim,\n",
    "    nhead=nhead,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_hidden_layers=num_hidden_layers,\n",
    "    num_layers=num_layers,\n",
    "    kernel_size=kernel_size,\n",
    "    dropout=dropout,\n",
    "    pooling=pooling,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-20 17:21:24,188 - clearml.model - WARNING - Connecting multiple input models with the same name: `failed_uploading`. This might result in the wrong model being used when executing remotely\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0.load_state_dict(torch.load(\"best_model_0.pth\"))\n",
    "model_1.load_state_dict(torch.load(\"best_model_1.pth\"))\n",
    "model_2.load_state_dict(torch.load(\"best_model_2.pth\"))\n",
    "model_3.load_state_dict(torch.load(\"best_model_3.pth\"))\n",
    "model_4.load_state_dict(torch.load(\"best_model_4.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_0, model_1, model_2, model_3, model_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on benchmark pdb2272"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embed = load_dict_from_hdf5(\n",
    "    \"../../../../ssd2/dbp_finder/ankh_embeddings/pdb2272_2d.h5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in test_embed:\n",
    "    test_embed[key] = np.squeeze(test_embed[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embed = pd.DataFrame(list(test_embed.items()), columns=[\"identifier\", \"embedding\"])\n",
    "test_df = pd.read_csv(\"../data/embeddings/input_csv/pdb2272.csv\")\n",
    "test_df = test_df.merge(test_embed, on=\"identifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1153\n",
       "0    1119\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_set = SequenceDataset(test_df)\n",
    "testing_dataloader = DataLoader(\n",
    "    testing_set,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=False,\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(testing_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fn(models, testing_dataloader):\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in testing_dataloader:\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "\n",
    "            y = y.unsqueeze(1)\n",
    "            ens_logits = []\n",
    "\n",
    "            for model in models:\n",
    "                model.eval()\n",
    "                model = model.to(DEVICE)\n",
    "                output = model(x, y)\n",
    "\n",
    "                logits = output.logits\n",
    "                ens_logits.append(logits)\n",
    "\n",
    "            ens_logits = torch.stack(ens_logits, dim=0)\n",
    "            ens_logits = torch.mean(ens_logits, dim=0)\n",
    "\n",
    "            all_logits.extend(ens_logits.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    all_preds = [1 if logit > 0.5 else 0 for logit in all_logits]\n",
    "    metrics = calculate_metrics(all_labels, all_preds, all_logits)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate_fn(models, testing_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics, index=[\"pdb2272\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.report_table(title=\"pdb2272\", series=\"Metrics\", table_plot=metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
