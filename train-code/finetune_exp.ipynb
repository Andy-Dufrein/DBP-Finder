{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ankh\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml\n",
    "import logging\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from data_prepare import make_folds, prepare_embed_df\n",
    "from torch_utils import (\n",
    "    SequenceDataset,\n",
    "    custom_collate_fn,\n",
    "    CustomBatchSampler,\n",
    "    train_fn,\n",
    "    validate_fn,\n",
    "    evaluate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clearml\n",
    "from clearml import Logger, Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=9b912b2281484adca0cba97e26b1a38d\n",
      "2024-06-20 14:05:45,131 - clearml.Task - INFO - Storing jupyter notebook directly as code\n",
      "ClearML results page: https://app.clear.ml/projects/45ba7ff7a93646a8a76d1950065cf1d5/experiments/9b912b2281484adca0cba97e26b1a38d/output/log\n",
      "ClearML Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start\n"
     ]
    }
   ],
   "source": [
    "clearml.browser_login()\n",
    "task = Task.init(\n",
    "    project_name=\"DBPs_search\",\n",
    "    task_name=\"Finetune Ankh v4\",\n",
    "    output_uri=True,\n",
    ")\n",
    "logger = Logger.current_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'task' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241m.\u001b[39mconnect_configuration(config)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'task' is not defined"
     ]
    }
   ],
   "source": [
    "task.connect_configuration(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = config[\"model_config\"][\"input_dim\"]\n",
    "nhead = config[\"model_config\"][\"nhead\"]\n",
    "hidden_dim = config[\"model_config\"][\"hidden_dim\"]\n",
    "num_hidden_layers = config[\"model_config\"][\"num_hidden_layers\"]\n",
    "num_layers = config[\"model_config\"][\"num_layers\"]\n",
    "kernel_size = config[\"model_config\"][\"kernel_size\"]\n",
    "dropout = config[\"model_config\"][\"dropout\"]\n",
    "pooling = config[\"model_config\"][\"pooling\"]\n",
    "\n",
    "\n",
    "epochs = config[\"training_config\"][\"epochs\"]\n",
    "lr = config[\"training_config\"][\"lr\"]\n",
    "factor = config[\"training_config\"][\"factor\"]\n",
    "patience = config[\"training_config\"][\"patience\"]\n",
    "min_lr = config[\"training_config\"][\"min_lr\"]\n",
    "batch_size = config[\"training_config\"][\"batch_size\"]\n",
    "seed = config[\"training_config\"][\"seed\"]\n",
    "num_workers = config[\"training_config\"][\"num_workers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train - pd dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_classification_model = ankh.ConvBertForBinaryClassification(\n",
    "#     input_dim=1536,\n",
    "#     nhead=7,\n",
    "#     hidden_dim=1723,\n",
    "#     num_hidden_layers=2,\n",
    "#     num_layers=1,\n",
    "#     kernel_size=7,\n",
    "#     dropout=dropout,\n",
    "#     pooling=pooling,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_classification_model = binary_classification_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a, b = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = binary_classification_model(a.to(DEVICE), b.to(DEVICE).unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = AdamW(binary_classification_model.parameters(), lr=float(lr))\n",
    "# scheduler = ReduceLROnPlateau(\n",
    "#     optimizer, mode=\"min\", factor=factor, patience=patience, min_lr=float(min_lr)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_embed_df(csv_path=\"../data/ready_data/train_pdb186.csv\")\n",
    "train_folds, valid_folds = make_folds(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    34325\n",
       "0    34325\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-20 14:15:22,329 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_0.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 14:15:22,336 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 14:19:39,146 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_0.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 14:19:39,154 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 14:27:31,679 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_0.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 14:27:31,687 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 14:42:22,558 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_0.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 14:42:22,564 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 14:51:35,174 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_0.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 14:51:35,197 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 15:00:05,229 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_1.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 15:00:05,236 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 15:07:19,512 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_1.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 15:07:19,518 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 15:10:42,779 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_1.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 15:10:42,784 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 15:34:08,872 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_2.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 15:34:08,879 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 15:37:30,184 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_2.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 15:37:30,190 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 15:50:48,926 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_2.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 15:50:48,932 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 15:54:24,533 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_2.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 15:54:24,538 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 16:07:49,668 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_3.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 16:07:49,673 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 16:11:20,799 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_3.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 16:11:20,804 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 16:14:53,645 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_3.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 16:14:53,651 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 16:37:58,138 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_3.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 16:37:58,144 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 16:41:27,304 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_4.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 16:41:27,310 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 16:45:03,568 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_4.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 16:45:03,574 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 16:55:09,013 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_4.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 16:55:09,019 - clearml.Task - INFO - Failed model upload\n",
      "2024-06-20 17:08:32,303 - clearml.storage - ERROR - Exception encountered while uploading Failed uploading object /DBPs_search/Finetune Ankh v3.9b912b2281484adca0cba97e26b1a38d/models/best_model_4.pth (413): <!doctype html>\n",
      "<html lang=en>\n",
      "<title>413 Request Entity Too Large</title>\n",
      "<h1>Request Entity Too Large</h1>\n",
      "<p>The data value transmitted exceeds the capacity limit.</p>\n",
      "\n",
      "2024-06-20 17:08:32,310 - clearml.Task - INFO - Failed model upload\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_folds)):\n",
    "    train_dataset = SequenceDataset(train_folds[i])\n",
    "    train_sampler = CustomBatchSampler(train_dataset, batch_size)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        num_workers=num_workers,\n",
    "        batch_sampler=train_sampler,\n",
    "        collate_fn=custom_collate_fn,\n",
    "    )\n",
    "\n",
    "    valid_dataset = SequenceDataset(valid_folds[i])\n",
    "    valid_sampler = CustomBatchSampler(valid_dataset, batch_size)\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        num_workers=num_workers,\n",
    "        batch_sampler=valid_sampler,\n",
    "        collate_fn=custom_collate_fn,\n",
    "    )\n",
    "\n",
    "    binary_classification_model = ankh.ConvBertForBinaryClassification(\n",
    "        input_dim=input_dim,\n",
    "        nhead=nhead,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_hidden_layers=num_hidden_layers,\n",
    "        num_layers=num_layers,\n",
    "        kernel_size=kernel_size,\n",
    "        dropout=dropout,\n",
    "        pooling=pooling,\n",
    "    )\n",
    "\n",
    "    binary_classification_model = binary_classification_model.to(DEVICE)\n",
    "    optimizer = AdamW(binary_classification_model.parameters(), lr=float(lr))\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", factor=factor, patience=patience, min_lr=float(min_lr)\n",
    "    )\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model_path = f\"checkpoints/pdb20000_best_model_{i}.pth\"\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_fn(binary_classification_model, train_dataloader, optimizer, DEVICE)\n",
    "        valid_loss, metrics = validate_fn(\n",
    "            binary_classification_model, valid_dataloader, scheduler, DEVICE\n",
    "        )\n",
    "\n",
    "        logger.report_scalar(\n",
    "            title=f\"Loss model {i}\",\n",
    "            series=\"train loss\",\n",
    "            value=train_loss,\n",
    "            iteration=epoch,\n",
    "        )\n",
    "        logger.report_scalar(\n",
    "            title=f\"Loss model {i}\",\n",
    "            series=\"valid loss\",\n",
    "            value=valid_loss,\n",
    "            iteration=epoch,\n",
    "        )\n",
    "\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            logger.report_scalar(\n",
    "                title=f\"Metrics model {i}\",\n",
    "                series=metric_name,\n",
    "                value=metric_value,\n",
    "                iteration=epoch,\n",
    "            )\n",
    "\n",
    "        if valid_loss < best_val_loss:\n",
    "            best_val_loss = valid_loss\n",
    "            torch.save(binary_classification_model.state_dict(), best_model_path)\n",
    "            message = f\"Saved Best Model on epoch {epoch} with Validation Loss: {best_val_loss}\"\n",
    "            logger.report_text(message, level=logging.DEBUG, print_console=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference average best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for i in range(5):\n",
    "    binary_classification_model = ankh.ConvBertForBinaryClassification(\n",
    "        input_dim=input_dim,\n",
    "        nhead=nhead,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_hidden_layers=num_hidden_layers,\n",
    "        num_layers=num_layers,\n",
    "        kernel_size=kernel_size,\n",
    "        dropout=dropout,\n",
    "        pooling=pooling,\n",
    "    )\n",
    "\n",
    "    path_model = f\"checkpoints/pdb2272_best_model_{i}.pth\"\n",
    "    binary_classification_model.load_state_dict(torch.load(path_model))\n",
    "    binary_classification_model.eval()  # Set the model to evaluation mode\n",
    "    models.append(binary_classification_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on benchmark pdb2272"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = prepare_embed_df(\n",
    "    embedding_path=\"../../../../ssd2/dbp_finder/ankh_embeddings/pdb186_2d.h5\",\n",
    "    csv_path=\"../data/embeddings/input_csv/pdb186.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    93\n",
       "0    93\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>sequence</th>\n",
       "      <th>label</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2L8DA</td>\n",
       "      <td>GAMGMPNRKYADGEVVMGRWPGSVLYYEVQVTSYDDASHLYTVKYK...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.016087042, -0.005283513, 0.008722133, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2LI6A</td>\n",
       "      <td>QSLNPALQEKISTELNNKQYELFMKSLIENCKKRNMPLQSIPEIGN...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.023384146, 0.009197159, -0.023114325, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2LJ6A</td>\n",
       "      <td>MPSSKPLAEYARKRDFRQTPEPSGRKPRKDSTGLLRYCVQKHDASR...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.014131906, 0.0005972403, -0.018032532, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2LUYA</td>\n",
       "      <td>HMGKNDNDALIMCMRCRKVKGIDSYSKTQWSKTFTFVRGRTVSVSD...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.028977185, -0.0040170737, -0.0045093386, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2LVSA</td>\n",
       "      <td>MPSVNDSLDIVEKLYKDGVPVKEIAKRSNNSMSTVYKALEKLEAMG...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.01739704, -0.003137477, 0.007429028, -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>4HWXA</td>\n",
       "      <td>GSAHGPSAMVFTVIQGSGEPTDTVLRATTLSCAYTAEGTHPAPRAA...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.04853192, -0.008373539, 0.0075779436, 0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>4HY3A</td>\n",
       "      <td>MHHHHHHSSGVDLGTENLYFQSMTNTERPLAISAPEPRSLDLIFSD...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.019954232, -0.033106994, 0.02529409, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>4I1DA</td>\n",
       "      <td>SNAQITFVSQGGAYQAAQTVAILDPSAKKLGITINQDSIPDAWPAI...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0064985356, -0.017093375, -0.009304281, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>4I8DA</td>\n",
       "      <td>AVVPPAGTPWGTAYDKAKAALAKLNLQDKVGIVSGVGWNGGPCVGN...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.011188501, -0.00968882, -0.022511976, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>4IAOC</td>\n",
       "      <td>MGSSHHHHHHSQDPNSMDPLAVSAASVVSMSNDVLKPETPKGPIII...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.01735797, 0.004733767, 0.014789492, 0.0157...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>186 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    identifier                                           sequence  label  \\\n",
       "0        2L8DA  GAMGMPNRKYADGEVVMGRWPGSVLYYEVQVTSYDDASHLYTVKYK...      1   \n",
       "1        2LI6A  QSLNPALQEKISTELNNKQYELFMKSLIENCKKRNMPLQSIPEIGN...      1   \n",
       "2        2LJ6A  MPSSKPLAEYARKRDFRQTPEPSGRKPRKDSTGLLRYCVQKHDASR...      1   \n",
       "3        2LUYA  HMGKNDNDALIMCMRCRKVKGIDSYSKTQWSKTFTFVRGRTVSVSD...      1   \n",
       "4        2LVSA  MPSVNDSLDIVEKLYKDGVPVKEIAKRSNNSMSTVYKALEKLEAMG...      1   \n",
       "..         ...                                                ...    ...   \n",
       "181      4HWXA  GSAHGPSAMVFTVIQGSGEPTDTVLRATTLSCAYTAEGTHPAPRAA...      0   \n",
       "182      4HY3A  MHHHHHHSSGVDLGTENLYFQSMTNTERPLAISAPEPRSLDLIFSD...      0   \n",
       "183      4I1DA  SNAQITFVSQGGAYQAAQTVAILDPSAKKLGITINQDSIPDAWPAI...      0   \n",
       "184      4I8DA  AVVPPAGTPWGTAYDKAKAALAKLNLQDKVGIVSGVGWNGGPCVGN...      0   \n",
       "185      4IAOC  MGSSHHHHHHSQDPNSMDPLAVSAASVVSMSNDVLKPETPKGPIII...      0   \n",
       "\n",
       "                                             embedding  \n",
       "0    [[0.016087042, -0.005283513, 0.008722133, 0.00...  \n",
       "1    [[0.023384146, 0.009197159, -0.023114325, -0.0...  \n",
       "2    [[0.014131906, 0.0005972403, -0.018032532, -0....  \n",
       "3    [[0.028977185, -0.0040170737, -0.0045093386, 0...  \n",
       "4    [[0.01739704, -0.003137477, 0.007429028, -0.00...  \n",
       "..                                                 ...  \n",
       "181  [[0.04853192, -0.008373539, 0.0075779436, 0.01...  \n",
       "182  [[-0.019954232, -0.033106994, 0.02529409, 0.00...  \n",
       "183  [[0.0064985356, -0.017093375, -0.009304281, 0....  \n",
       "184  [[0.011188501, -0.00968882, -0.022511976, -0.0...  \n",
       "185  [[0.01735797, 0.004733767, 0.014789492, 0.0157...  \n",
       "\n",
       "[186 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_set = SequenceDataset(test_df)\n",
    "testing_dataloader = DataLoader(\n",
    "    testing_set,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=False,\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate_fn(models, testing_dataloader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics, index=[\"pdb20000\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.report_table(title=\"pdb20000\", series=\"Metrics\", table_plot=metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
