{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ankh\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml\n",
    "from sklearn.metrics import (accuracy_score, f1_score, matthews_corrcoef,\n",
    "                             precision_score, recall_score, roc_auc_score)\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import (BatchSampler, DataLoader, Dataset,\n",
    "                              SequentialSampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clearml\n",
    "from clearml import Logger, Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clearml.browser_login()\n",
    "task = Task.init(\n",
    "    project_name=\"DBPs_search\",\n",
    "    task_name=\"Finetune Ankh v2\",\n",
    "    output_uri=True,\n",
    ")\n",
    "logger = Logger.current_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_config': {'input_dim': 1536,\n",
       "  'nhead': 4,\n",
       "  'hidden_dim': 1536,\n",
       "  'num_hidden_layers': 1,\n",
       "  'num_layers': 1,\n",
       "  'kernel_size': 7,\n",
       "  'dropout': 0.2,\n",
       "  'pooling': 'max'},\n",
       " 'training_config': {'epochs': 10,\n",
       "  'lr': '2e-4',\n",
       "  'seed': 42,\n",
       "  'factor': 0.5,\n",
       "  'patience': 2,\n",
       "  'min_lr': '1e-6',\n",
       "  'batch_size': 128,\n",
       "  'num_workers': 4,\n",
       "  'optimizer': 'adamw'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.connect_configuration(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = config[\"model_config\"][\"input_dim\"]\n",
    "nhead = config[\"model_config\"][\"nhead\"]\n",
    "hidden_dim = config[\"model_config\"][\"hidden_dim\"]\n",
    "num_hidden_layers = config[\"model_config\"][\"num_hidden_layers\"]\n",
    "num_layers = config[\"model_config\"][\"num_layers\"]\n",
    "kernel_size = config[\"model_config\"][\"kernel_size\"]\n",
    "dropout = config[\"model_config\"][\"dropout\"]\n",
    "pooling = config[\"model_config\"][\"pooling\"]\n",
    "\n",
    "\n",
    "epochs = config[\"training_config\"][\"epochs\"]\n",
    "lr = config[\"training_config\"][\"lr\"]\n",
    "factor = config[\"training_config\"][\"factor\"]\n",
    "patience = config[\"training_config\"][\"patience\"]\n",
    "min_lr = config[\"training_config\"][\"min_lr\"]\n",
    "batch_size = config[\"training_config\"][\"batch_size\"]\n",
    "seed = config[\"training_config\"][\"seed\"]\n",
    "num_workers = config[\"training_config\"][\"num_workers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict_from_hdf5(filename):\n",
    "    \"\"\"\n",
    "    Load a dictionary with string keys and NumPy array values from an HDF5 file.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): Name of the HDF5 file to load the data from.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary with string keys and NumPy array values.\n",
    "    \"\"\"\n",
    "    loaded_dict = {}\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        for key in f.keys():\n",
    "            loaded_dict[key] = f[key][:]\n",
    "    return loaded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(\n",
    "    all_labels: list, all_preds: list, logits: list\n",
    ") -> dict[str, float]:\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    logits = np.array(logits)\n",
    "\n",
    "    auc = roc_auc_score(all_labels, logits)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    specificity = recall_score(all_labels, all_preds, pos_label=0)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    metrics = {\n",
    "        \"auc\": auc,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"mcc\": mcc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"specificity\": specificity,\n",
    "        \"f1_score\": f1,\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mload_dict_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/embeddings/ankh_embeddings/train_p2_2d.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 14\u001b[0m, in \u001b[0;36mload_dict_from_hdf5\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m---> 14\u001b[0m         loaded_dict[key] \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_dict\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/bind_predict/lib/python3.11/site-packages/h5py/_hl/dataset.py:758\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, args, new_dtype)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fast_read_ok \u001b[38;5;129;01mand\u001b[39;00m (new_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 758\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fast_reader\u001b[38;5;241m.\u001b[39mread(args)\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    760\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall back to Python read pathway below\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embeddings = load_dict_from_hdf5(\"../data/embeddings/ankh_embeddings/train_p2_2d.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in embeddings:\n",
    "    embeddings[key] = np.squeeze(embeddings[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_df = pd.DataFrame(list(embeddings.items()), columns=[\"identifier\", \"embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/ready_data/train_pdb2272.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(embed_df, on=\"identifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(368, 1536)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.embedding.iloc[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "368"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.embedding.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gkf = GroupKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df[\"sequence\"].tolist()\n",
    "y = train_df[\"label\"].tolist()\n",
    "groups = train_df[\"cluster\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_idx, test_idx in gkf.split(X, y, groups=groups):\n",
    "    train_idx = train_idx.tolist()\n",
    "    test_idx = test_idx.tolist()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df.iloc[train_idx]\n",
    "test = train_df.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.embeds = df.embedding.tolist()\n",
    "        self.labels = df.label.tolist()\n",
    "        self.lengths = [len(embed) for embed in self.embeds]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeds)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.embeds[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        y = torch.tensor(y, dtype=torch.float)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    # Extract the embeddings from the batch\n",
    "    embeddings = [item[0] for item in batch]\n",
    "    labels = torch.tensor([item[1] for item in batch], dtype=torch.float)\n",
    "\n",
    "    # Pad the embeddings\n",
    "    padded_embeddings = pad_sequence(embeddings, batch_first=True)\n",
    "    return padded_embeddings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBatchSampler(BatchSampler):\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.sampler = SequentialSampler(dataset)\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = list(self.sampler)\n",
    "        indices.sort(\n",
    "            key=lambda i: self.dataset.lengths[i], reverse=True\n",
    "        )  # Sort indices by sequence length\n",
    "        batches = [\n",
    "            indices[i : i + self.batch_size]\n",
    "            for i in range(0, len(indices), self.batch_size)\n",
    "        ]\n",
    "        for batch in batches:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SequenceDataset(train)\n",
    "train_sampler = CustomBatchSampler(train_dataset, batch_size)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    num_workers=num_workers,\n",
    "    batch_sampler=train_sampler,\n",
    "    collate_fn=custom_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SequenceDataset(test)\n",
    "test_sampler = CustomBatchSampler(test_dataset, batch_size)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    num_workers=num_workers,\n",
    "    batch_sampler=test_sampler,\n",
    "    collate_fn=custom_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_classification_model = ankh.ConvBertForBinaryClassification(\n",
    "    input_dim=input_dim,\n",
    "    nhead=nhead,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_hidden_layers=num_hidden_layers,\n",
    "    num_layers=num_layers,\n",
    "    kernel_size=kernel_size,\n",
    "    dropout=dropout,\n",
    "    pooling=pooling,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_classification_model = binary_classification_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a, b = next(iter(train_dataloader))\n",
    "# output = binary_classification_model(a.to(DEVICE), b.to(DEVICE).unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(binary_classification_model.parameters(), lr=float(lr))\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=factor, patience=patience, min_lr=float(min_lr)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(binary_classification_model, train_dataloader, optimizer):\n",
    "    binary_classification_model.train()\n",
    "    loss = 0.0\n",
    "    for x, y in train_dataloader:\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "\n",
    "        y = y.unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = binary_classification_model(x, y)\n",
    "        output.loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss += output.loss.item()\n",
    "\n",
    "    epoch_loss = loss / len(train_dataloader)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(binary_classification_model, test_dataloader, scheduler):\n",
    "    binary_classification_model.eval()\n",
    "    loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    logits = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_dataloader:\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "\n",
    "            y = y.unsqueeze(1)\n",
    "\n",
    "            output = binary_classification_model(x, y)\n",
    "            loss += output.loss.item()\n",
    "\n",
    "            preds = (output.logits > 0.5).float()\n",
    "\n",
    "            logits.extend(output.logits.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    epoch_loss = loss / len(test_dataloader)\n",
    "    scheduler.step(epoch_loss)\n",
    "    metrics = calculate_metrics(all_labels, all_preds, logits)\n",
    "    return epoch_loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss = train(binary_classification_model, train_dataloader, optimizer)\n",
    "    valid_loss, metrics = validate(\n",
    "        binary_classification_model, test_dataloader, scheduler\n",
    "    )\n",
    "\n",
    "    logger.report_scalar(\n",
    "        title=\"Loss\", series=\"train loss\", value=train_loss, iteration=epoch\n",
    "    )\n",
    "    logger.report_scalar(\n",
    "        title=\"Loss\", series=\"valid loss\", value=valid_loss, iteration=epoch\n",
    "    )\n",
    "\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        logger.report_scalar(\n",
    "            title=\"Metrics\", series=metric_name, value=metric_value, iteration=epoch\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on benchmark pdb2272"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bind_predict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
