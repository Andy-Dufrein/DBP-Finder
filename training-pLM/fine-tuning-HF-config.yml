training_config:
  epochs: 10
  lr: 2e-4
  seed: 42
  factor: 0.1
  patience: 4
  min_lr: 1e-8
  batch_size: 64
  num_workers: 4
  optimizer: adamw
  weight_decay: 1e-2


lora_config:
  lora_alpha: 16
  lora_dropout: 0.1
  r: 16
